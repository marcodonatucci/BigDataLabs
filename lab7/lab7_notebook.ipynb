{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8c942f4",
   "metadata": {},
   "source": [
    "# Lab 7 - Barcelona Bike Sharing Station Criticality Analysis\n",
    "\n",
    "This notebook analyzes historical data about Barcelona bike sharing stations to identify the most \"critical\" timeslot for each station and generates a KML file for map visualization.\n",
    "\n",
    "## Objectives:\n",
    "\n",
    "1. **Analyze station criticality** by timeslot (day of week + hour)\n",
    "2. **Calculate criticality values** as ratio of full station readings to total readings\n",
    "3. **Filter by minimum criticality threshold** (command line argument)\n",
    "4. **Select most critical timeslot** for each station\n",
    "5. **Generate KML output** for map visualization\n",
    "\n",
    "## Input Files:\n",
    "\n",
    "1. **register.csv**: Historical station data\n",
    "   - Format: `stationId\\ttimestamp\\tusedslots\\tfreeslots`\n",
    "   - Example: `23 2008-05-15 19:01:00 5 13`\n",
    "\n",
    "2. **stations.csv**: Station location data\n",
    "   - Format: `stationId\\tlongitude\\tlatitude\\tname`\n",
    "   - Example: `1 2.180019 41.397978 Gran Via Corts Catalanes`\n",
    "\n",
    "## Criticality Definition:\n",
    "A station is \"critical\" when free_slots = 0 (station is full).\n",
    "\n",
    "Criticality = (Number of readings with free_slots = 0) / (Total readings for station-timeslot pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3b4855",
   "metadata": {},
   "source": [
    "## Import libraries and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b080625",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict, List\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from datetime import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32f7c8b",
   "metadata": {},
   "source": [
    "## Parameters configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a8af2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration of paths and parameters\n",
    "registerPath = \"sampleData/registerSample.csv\"  # For local testing\n",
    "stationsPath = \"sampleData/stations.csv\"        # For local testing\n",
    "# registerPath = \"/data/students/bigdata-01QYD/Lab7/register.csv\"  # For HDFS\n",
    "# stationsPath = \"/data/students/bigdata-01QYD/Lab7/stations.csv\"  # For HDFS\n",
    "\n",
    "outputPath = \"critical_stations_output/\"\n",
    "kmlOutputPath = \"critical_stations.kml\"\n",
    "\n",
    "# Minimum criticality threshold (can be set as command line argument)\n",
    "minCriticalityThreshold = 0.3  # Example: 30%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f432a3d6",
   "metadata": {},
   "source": [
    "## Read and preprocess station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3849b668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stations file\n",
    "stationsRDD = sc.textFile(stationsPath)\n",
    "\n",
    "# Remove header\n",
    "stationsHeader = stationsRDD.first()\n",
    "\n",
    "stationsDataRDD = stationsRDD.filter(lambda line: line != stationsHeader)\n",
    "\n",
    "def parse_station_line(line: str) -> Tuple[int, Tuple[float, float, str]]:\n",
    "    \"\"\"\n",
    "    Parse station line: id\\tlongitude\\tlatitude\\tname\n",
    "    Returns (stationId, (longitude, latitude, name))\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fields = line.split('\\t')\n",
    "        if len(fields) >= 4:\n",
    "            station_id = int(fields[0].strip())\n",
    "            longitude = float(fields[1].strip())\n",
    "            latitude = float(fields[2].strip())\n",
    "            name = fields[3].strip()\n",
    "            return (station_id, (longitude, latitude, name))\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Parse station data and create lookup dictionary\n",
    "stationsLookupRDD = stationsDataRDD.map(parse_station_line).filter(lambda x: x is not None)\n",
    "\n",
    "# Cache for joins\n",
    "stationsLookupRDD.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ca6c57",
   "metadata": {},
   "source": [
    "## Read and preprocess register data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c359462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read register file\n",
    "registerRDD = sc.textFile(registerPath)\n",
    "\n",
    "# Remove header\n",
    "registerHeader = registerRDD.first()\n",
    "\n",
    "registerDataRDD = registerRDD.filter(lambda line: line != registerHeader)\n",
    "\n",
    "def parse_register_line(line: str) -> Tuple[int, str, int, int]:\n",
    "    \"\"\"\n",
    "    Parse register line: stationId\\ttimestamp\\tusedslots\\tfreeslots\n",
    "    Returns (stationId, timestamp, usedSlots, freeSlots)\n",
    "    Filter out invalid data (used=0 and free=0)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fields = line.split('\\t')\n",
    "        if len(fields) >= 4:\n",
    "            station_id = int(fields[0].strip())\n",
    "            timestamp = fields[1].strip()\n",
    "            used_slots = int(fields[2].strip())\n",
    "            free_slots = int(fields[3].strip())\n",
    "            \n",
    "            # Filter out invalid readings (both used and free = 0)\n",
    "            if used_slots == 0 and free_slots == 0:\n",
    "                return None\n",
    "                \n",
    "            return (station_id, timestamp, used_slots, free_slots)\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Parse register data\n",
    "validRegisterRDD = registerDataRDD.map(parse_register_line).filter(lambda x: x is not None)\n",
    "\n",
    "# Cache for multiple operations\n",
    "validRegisterRDD.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7311602b",
   "metadata": {},
   "source": [
    "## Extract timeslots (day of week + hour) from timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dd59f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_timeslot(timestamp: str) -> Tuple[str, int]:\n",
    "    \"\"\"\n",
    "    Extract day of week and hour from timestamp\n",
    "    Returns (dayOfWeek, hour)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse timestamp: \"2008-05-15 12:01:00\"\n",
    "        datetime_obj = datetime.strptime(timestamp, \"%Y-%m-%d %H:%M:%S\")\n",
    "        day_of_week = datetime_obj.strftime(\"%a\")  # Mon, Tue, Wed, etc.\n",
    "        hour = datetime_obj.hour\n",
    "        return (day_of_week, hour)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def create_station_timeslot_data(reading: Tuple[int, str, int, int]):\n",
    "    \"\"\"\n",
    "    Convert reading to ((stationId, timeslot), (isCritical, 1))\n",
    "    where isCritical = 1 if free_slots == 0, else 0\n",
    "    \"\"\"\n",
    "    station_id, timestamp, used_slots, free_slots = reading\n",
    "    \n",
    "    timeslot = extract_timeslot(timestamp)\n",
    "    if timeslot is None:\n",
    "        return None\n",
    "    \n",
    "    day_of_week, hour = timeslot\n",
    "    is_critical = 1 if free_slots == 0 else 0\n",
    "    \n",
    "    return ((station_id, (day_of_week, hour)), (is_critical, 1))\n",
    "\n",
    "# Transform readings to station-timeslot pairs with criticality info\n",
    "stationTimeslotRDD = validRegisterRDD.map(create_station_timeslot_data).filter(lambda x: x is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c812ab",
   "metadata": {},
   "source": [
    "## Calculate criticality for each station-timeslot pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddae2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data: sum critical readings and total readings for each (station, timeslot)\n",
    "aggregatedRDD = stationTimeslotRDD.reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "\n",
    "def calculate_criticality(data):\n",
    "    \"\"\"\n",
    "    Calculate criticality ratio from aggregated data\n",
    "    Returns ((stationId, timeslot), criticalityValue)\n",
    "    \"\"\"\n",
    "    (station_id, timeslot), (critical_count, total_count) = data\n",
    "    \n",
    "    if total_count == 0:\n",
    "        return None\n",
    "    \n",
    "    criticality = critical_count / total_count\n",
    "    return ((station_id, timeslot), criticality)\n",
    "\n",
    "# Calculate criticality for each station-timeslot pair\n",
    "criticalityRDD = aggregatedRDD.map(calculate_criticality).filter(lambda x: x is not None)\n",
    "\n",
    "# Cache for multiple operations\n",
    "criticalityRDD.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aca791",
   "metadata": {},
   "source": [
    "## Filter by minimum criticality threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50611650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter pairs with criticality >= threshold\n",
    "filteredCriticalityRDD = criticalityRDD.filter(lambda x: x[1] >= minCriticalityThreshold)\n",
    "\n",
    "# Cache for multiple operations\n",
    "filteredCriticalityRDD.cache()\n",
    "\n",
    "filteredCount = filteredCriticalityRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29d8878",
   "metadata": {},
   "source": [
    "## Select most critical timeslot for each station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90610751",
   "metadata": {},
   "outputs": [],
   "source": [
    "if filteredCount > 0:\n",
    "    # Transform to (stationId, (timeslot, criticality)) for grouping by station\n",
    "    stationGroupedRDD = filteredCriticalityRDD.map(lambda x: (x[0][0], (x[0][1], x[1])))\n",
    "    \n",
    "    def select_most_critical_timeslot(timeslots_data):\n",
    "        \"\"\"\n",
    "        Select most critical timeslot for a station\n",
    "        If tied, select earliest hour, then lexicographical day order\n",
    "        \"\"\"\n",
    "        station_id, timeslots = timeslots_data\n",
    "        timeslots_list = list(timeslots)\n",
    "        \n",
    "        if not timeslots_list:\n",
    "            return None\n",
    "        \n",
    "        # Sort by: 1) criticality (desc), 2) hour (asc), 3) day (lexicographical)\n",
    "        def sort_key(item):\n",
    "            (day, hour), criticality = item\n",
    "            return (-criticality, hour, day)  # negative criticality for descending order\n",
    "        \n",
    "        sorted_timeslots = sorted(timeslots_list, key=sort_key)\n",
    "        most_critical = sorted_timeslots[0]\n",
    "        \n",
    "        (day, hour), criticality = most_critical\n",
    "        return (station_id, (day, hour, criticality))\n",
    "    \n",
    "    # Group by station and select most critical timeslot\n",
    "    mostCriticalRDD = stationGroupedRDD.groupByKey().map(select_most_critical_timeslot).filter(lambda x: x is not None)\n",
    "    \n",
    "    # Cache for output operations\n",
    "    mostCriticalRDD.cache()\n",
    "    \n",
    "else:\n",
    "    print(\"No data to process for most critical timeslot selection\")\n",
    "    mostCriticalRDD = sc.emptyRDD()\n",
    "    stationsWithCriticalTimeslots = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecd2dba",
   "metadata": {},
   "source": [
    "## Join with station location data and generate KML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7faf0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if stationsWithCriticalTimeslots > 0:\n",
    "    # Join most critical timeslots with station location data\n",
    "    # mostCriticalRDD: (stationId, (day, hour, criticality))\n",
    "    # stationsLookupRDD: (stationId, (longitude, latitude, name))\n",
    "    joinedRDD = mostCriticalRDD.join(stationsLookupRDD)\n",
    "    \n",
    "    def generate_kml_placemark(data):\n",
    "        \"\"\"\n",
    "        Generate KML Placemark for a station\n",
    "        \"\"\"\n",
    "        station_id, ((day, hour, criticality), (longitude, latitude, name)) = data\n",
    "        \n",
    "        # Format KML Placemark\n",
    "        kml_line = (\n",
    "            f'<Placemark><name>{station_id}</name><ExtendedData>'\n",
    "            f'<Data name=\"DayWeek\"><value>{day}</value></Data>'\n",
    "            f'<Data name=\"Hour\"><value>{hour}</value></Data>'\n",
    "            f'<Data name=\"Criticality\"><value>{criticality}</value></Data></ExtendedData>'\n",
    "            f'<Point><coordinates>{longitude},{latitude}</coordinates></Point></Placemark>'\n",
    "        )\n",
    "        return kml_line\n",
    "    \n",
    "    # Generate KML placemarks\n",
    "    kmlRDD = joinedRDD.map(generate_kml_placemark)\n",
    "    \n",
    "    # Coalesce to single partition for single output file\n",
    "    singlePartitionKmlRDD = kmlRDD.coalesce(1)\n",
    "        \n",
    "else:\n",
    "    print(\"No data available for KML generation\")\n",
    "    kmlRDD = sc.emptyRDD()\n",
    "    kmlEntries = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59b0174",
   "metadata": {},
   "source": [
    "## Save results to output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275192fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if kmlEntries > 0:\n",
    "    try:\n",
    "        # Save KML data\n",
    "        singlePartitionKmlRDD.saveAsTextFile(outputPath)\n",
    "        print(f\"\\nKML data saved successfully to: {outputPath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving to {outputPath}: {e}\")\n",
    "        print(\"Note: You may need to delete the output folder if it already exists\")\n",
    "else:\n",
    "    print(f\"\\nNo KML data to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9ea8b2",
   "metadata": {},
   "source": [
    "## Sample KML file structure\n",
    "\n",
    "To create a complete KML file for map visualization, wrap the generated placemarks in this structure:\n",
    "\n",
    "```xml\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<kml xmlns=\"http://www.opengis.net/kml/2.2\">\n",
    "<Document>\n",
    "  <name>Barcelona Bike Sharing Critical Stations</name>\n",
    "  <description>Stations with critical timeslots (high occupancy)</description>\n",
    "  \n",
    "  <!-- Insert generated placemarks here -->\n",
    "  \n",
    "</Document>\n",
    "</kml>\n",
    "```\n",
    "\n",
    "Each placemark contains:\n",
    "- **Station ID** as name\n",
    "- **Day of week** and **hour** of most critical timeslot\n",
    "- **Criticality value** (0.0 to 1.0)\n",
    "- **GPS coordinates** for map positioning\n",
    "\n",
    "**Visualization tools:**\n",
    "- [KML Viewer](https://kmlviewer.nsspot.net)\n",
    "- [GPS Visualizer](https://www.gpsvisualizer.com)\n",
    "- Google Earth\n",
    "- Google My Maps"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
