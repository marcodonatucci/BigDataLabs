{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7c116c8",
   "metadata": {},
   "source": [
    "# Lab 9 - Amazon Reviews Classification with Spark ML\n",
    "\n",
    "This notebook implements a machine learning pipeline to classify Amazon food reviews as \"useful\" or \"useless\" based on helpfulness ratings.\n",
    "\n",
    "## Objectives:\n",
    "\n",
    "1. **Step 1**: Preprocessing - Load and clean the dataset\n",
    "2. **Step 2**: Create Pipeline with simple features (text length)\n",
    "3. **Step 3**: Add more features to improve classification\n",
    "4. **Step 4**: Use text content for classification\n",
    "\n",
    "## Classification Task:\n",
    "- **Target**: Binary classification (useful vs useless)\n",
    "- **Useful**: HelpfulnessNumerator/HelpfulnessDenominator > 0.9 (90%)\n",
    "- **Useless**: HelpfulnessNumerator/HelpfulnessDenominator â‰¤ 0.9\n",
    "- **Filter**: Only reviews with HelpfulnessDenominator > 0\n",
    "\n",
    "## Algorithms to Compare:\n",
    "- **Decision Tree**\n",
    "- **Logistic Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d64c09",
   "metadata": {},
   "source": [
    "## Import libraries and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaeca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, IndexToString, SQLTransformer\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, RegexTokenizer, StopWordsRemover\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.functions import udf, length, col, when, split, size, regexp_count\n",
    "from pyspark.sql.types import LongType, BooleanType, DoubleType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5d0345",
   "metadata": {},
   "source": [
    "## Configuration and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d29ffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input path and constants\n",
    "inputData = \"ReviewsSample-2.csv\"  # For local testing\n",
    "# inputData = \"/data/students/bigdata-01QYD/Lab9/Reviews.csv\"  # For HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801acb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# Create a DataFrame from Reviews.csv\n",
    "reviews = spark.read.load(inputData,\n",
    "                     format=\"csv\",\n",
    "                     header=True,\n",
    "                     inferSchema=True)\n",
    "\n",
    "print(f\"Total reviews loaded: {reviews.count()}\")\n",
    "print(\"\\nSchema:\")\n",
    "reviews.printSchema()\n",
    "\n",
    "print(\"\\nFirst 3 reviews:\")\n",
    "reviews.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8470b340",
   "metadata": {},
   "source": [
    "## Step 1: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91eea26",
   "metadata": {},
   "source": [
    "### Filter rated reviews and create labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadfa140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the records with HelpfulnessDenominator>0 (i.e., rated reviews)\n",
    "rated_reviews = reviews.filter(col(\"HelpfulnessDenominator\") > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647f8128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and compute the value of Column label for the selected rated reviews\n",
    "# Label = 1.0 if helpfulness ratio > 0.9 (useful), else 0.0 (useless)\n",
    "labeled_reviews = rated_reviews.withColumn(\n",
    "    \"helpfulness_ratio\", \n",
    "    col(\"HelpfulnessNumerator\") / col(\"HelpfulnessDenominator\")\n",
    ").withColumn(\n",
    "    \"label\",\n",
    "    when(col(\"helpfulness_ratio\") > 0.9, 1.0).otherwise(0.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85975401",
   "metadata": {},
   "source": [
    "### Split data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccd893c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe with Column label in training and test set\n",
    "(reviews_train, reviews_test) = labeled_reviews.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Cache for performance\n",
    "reviews_train.cache()\n",
    "reviews_test.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190e778c",
   "metadata": {},
   "source": [
    "## Step 2: Simple Pipeline with Text Length Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f378248",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7ba7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create/Define the preprocessing steps and the classification algorithm\n",
    "# Implement a first solution with one single value in features: text length\n",
    "\n",
    "# SQL Transformer to add text length feature\n",
    "text_length_transformer = SQLTransformer(\n",
    "    statement=\"SELECT *, length(Text) as text_length FROM __THIS__\"\n",
    ")\n",
    "\n",
    "# Vector assembler to create features vector\n",
    "vector_assembler_simple = VectorAssembler(\n",
    "    inputCols=[\"text_length\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Decision Tree classifier\n",
    "dt_classifier = DecisionTreeClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline_dt_simple = Pipeline(stages=[\n",
    "    text_length_transformer,\n",
    "    vector_assembler_simple,\n",
    "    dt_classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfe28a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit/Train the model\n",
    "model_dt_simple = pipeline_dt_simple.fit(reviews_train)\n",
    "\n",
    "# Apply the model on the test set\n",
    "predictions_dt_simple = model_dt_simple.transform(reviews_test).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bfbc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics for Decision Tree with simple features\n",
    "# Accuracy, F1, weighted recall, weighted precision\n",
    "evaluatorAcc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "evaluatorF1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "evaluatorRecall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "evaluatorPrecision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "\n",
    "dt_simple_accuracy = evaluatorAcc.evaluate(predictions_dt_simple)\n",
    "dt_simple_f1 = evaluatorF1.evaluate(predictions_dt_simple)\n",
    "dt_simple_recall = evaluatorRecall.evaluate(predictions_dt_simple)\n",
    "dt_simple_precision = evaluatorPrecision.evaluate(predictions_dt_simple)\n",
    "\n",
    "print(f\"Accuracy: {dt_simple_accuracy:.4f}\")\n",
    "print(f\"F1: {dt_simple_f1:.4f}\")\n",
    "print(f\"Weighted Recall: {dt_simple_recall:.4f}\")\n",
    "print(f\"Weighted Precision: {dt_simple_precision:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837aac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix for Decision Tree\n",
    "#                     Predicted  \n",
    "#  Actual       Useful   Useless\n",
    "#  Useful          A        B\n",
    "#  Useless         C        D\n",
    "\n",
    "A_dt = predictions_dt_simple.filter(\"prediction=1 and label=1\").count()\n",
    "B_dt = predictions_dt_simple.filter(\"prediction=0 and label=1\").count()\n",
    "C_dt = predictions_dt_simple.filter(\"prediction=1 and label=0\").count()\n",
    "D_dt = predictions_dt_simple.filter(\"prediction=0 and label=0\").count()\n",
    "\n",
    "print(\"\\nConfusion Matrix (Decision Tree):\")\n",
    "print(\"                       Predicted\")\n",
    "print(\"  Actual \\t Useful\\tUseless\")\n",
    "print(f\"  Useful \\t {A_dt}\\t\\t{B_dt}\")\n",
    "print(f\"  Useless \\t {C_dt}\\t\\t{D_dt}\")\n",
    "\n",
    "# Precision and recall for the two classes\n",
    "# Useful\n",
    "if A_dt + C_dt == 0:\n",
    "    print(\"Precision(Useful): undefined\")\n",
    "else:\n",
    "    print(f\"Precision(Useful): {A_dt/(A_dt+C_dt):.4f}\")\n",
    "\n",
    "if A_dt + B_dt == 0:\n",
    "    print(\"Recall(Useful): undefined\")\n",
    "else:\n",
    "    print(f\"Recall(Useful): {A_dt/(A_dt+B_dt):.4f}\")\n",
    "\n",
    "# Useless \n",
    "if B_dt + D_dt == 0:\n",
    "    print(\"Precision(Useless): undefined\")\n",
    "else:\n",
    "    print(f\"Precision(Useless): {D_dt/(B_dt+D_dt):.4f}\")\n",
    "\n",
    "if C_dt + D_dt == 0:\n",
    "    print(\"Recall(Useless): undefined\")\n",
    "else:\n",
    "    print(f\"Recall(Useless): {D_dt/(C_dt+D_dt):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f3369d",
   "metadata": {},
   "source": [
    "### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79beff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression classifier\n",
    "lr_classifier = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "# Create pipeline with Logistic Regression\n",
    "pipeline_lr_simple = Pipeline(stages=[\n",
    "    text_length_transformer,\n",
    "    vector_assembler_simple,\n",
    "    lr_classifier\n",
    "])\n",
    "\n",
    "# Fit/Train the model\n",
    "model_lr_simple = pipeline_lr_simple.fit(reviews_train)\n",
    "\n",
    "# Apply the model on the test set\n",
    "predictions_lr_simple = model_lr_simple.transform(reviews_test).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df3219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics for Logistic Regression with simple features\n",
    "lr_simple_accuracy = evaluatorAcc.evaluate(predictions_lr_simple)\n",
    "lr_simple_f1 = evaluatorF1.evaluate(predictions_lr_simple)\n",
    "lr_simple_recall = evaluatorRecall.evaluate(predictions_lr_simple)\n",
    "lr_simple_precision = evaluatorPrecision.evaluate(predictions_lr_simple)\n",
    "\n",
    "print(f\"Accuracy: {lr_simple_accuracy:.4f}\")\n",
    "print(f\"F1: {lr_simple_f1:.4f}\")\n",
    "print(f\"Weighted Recall: {lr_simple_recall:.4f}\")\n",
    "print(f\"Weighted Precision: {lr_simple_precision:.4f}\")\n",
    "\n",
    "# Confusion matrix for Logistic Regression\n",
    "A_lr = predictions_lr_simple.filter(\"prediction=1 and label=1\").count()\n",
    "B_lr = predictions_lr_simple.filter(\"prediction=0 and label=1\").count()\n",
    "C_lr = predictions_lr_simple.filter(\"prediction=1 and label=0\").count()\n",
    "D_lr = predictions_lr_simple.filter(\"prediction=0 and label=0\").count()\n",
    "\n",
    "print(\"\\nConfusion Matrix (Logistic Regression):\")\n",
    "print(\"                       Predicted\")\n",
    "print(\"  Actual \\t Useful\\tUseless\")\n",
    "print(f\"  Useful \\t {A_lr}\\t\\t{B_lr}\")\n",
    "print(f\"  Useless \\t {C_lr}\\t\\t{D_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9245ac1e",
   "metadata": {},
   "source": [
    "## Step 3: Enhanced Features Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f9c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple features based on the review characteristics\n",
    "enhanced_features_transformer = SQLTransformer(\n",
    "    statement=\"\"\"\n",
    "    SELECT *,\n",
    "           length(Text) as text_length,\n",
    "           length(Summary) as summary_length,\n",
    "           Score as review_score,\n",
    "           HelpfulnessNumerator as helpful_votes,\n",
    "           HelpfulnessDenominator as total_votes,\n",
    "           CASE WHEN length(Text) > 100 THEN 1.0 ELSE 0.0 END as is_long_review,\n",
    "           CASE WHEN Score >= 4 THEN 1.0 ELSE 0.0 END as is_positive_score\n",
    "    FROM __THIS__\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Vector assembler with multiple features\n",
    "vector_assembler_enhanced = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"text_length\",\n",
    "        \"summary_length\", \n",
    "        \"review_score\",\n",
    "        \"helpful_votes\",\n",
    "        \"total_votes\",\n",
    "        \"is_long_review\",\n",
    "        \"is_positive_score\"\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51725d81",
   "metadata": {},
   "source": [
    "### Decision Tree with Enhanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493f54b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree with enhanced features\n",
    "pipeline_dt_enhanced = Pipeline(stages=[\n",
    "    enhanced_features_transformer,\n",
    "    vector_assembler_enhanced,\n",
    "    dt_classifier\n",
    "])\n",
    "\n",
    "# Train and evaluate\n",
    "model_dt_enhanced = pipeline_dt_enhanced.fit(reviews_train)\n",
    "predictions_dt_enhanced = model_dt_enhanced.transform(reviews_test).cache()\n",
    "\n",
    "print(\"\\n=== Decision Tree with Enhanced Features - Results ===\")\n",
    "dt_enhanced_accuracy = evaluatorAcc.evaluate(predictions_dt_enhanced)\n",
    "dt_enhanced_f1 = evaluatorF1.evaluate(predictions_dt_enhanced)\n",
    "dt_enhanced_recall = evaluatorRecall.evaluate(predictions_dt_enhanced)\n",
    "dt_enhanced_precision = evaluatorPrecision.evaluate(predictions_dt_enhanced)\n",
    "\n",
    "print(f\"Accuracy: {dt_enhanced_accuracy:.4f}\")\n",
    "print(f\"F1: {dt_enhanced_f1:.4f}\")\n",
    "print(f\"Weighted Recall: {dt_enhanced_recall:.4f}\")\n",
    "print(f\"Weighted Precision: {dt_enhanced_precision:.4f}\")\n",
    "\n",
    "# Show feature importance if available\n",
    "try:\n",
    "    feature_names = vector_assembler_enhanced.getInputCols()\n",
    "    dt_model = model_dt_enhanced.stages[2]  # Get the trained decision tree\n",
    "    importances = dt_model.featureImportances.toArray()\n",
    "    \n",
    "    print(\"\\nFeature Importances:\")\n",
    "    for name, importance in zip(feature_names, importances):\n",
    "        print(f\"  {name}: {importance:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not extract feature importances: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08742402",
   "metadata": {},
   "source": [
    "### Logistic Regression with Enhanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56313884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression with enhanced features\n",
    "pipeline_lr_enhanced = Pipeline(stages=[\n",
    "    enhanced_features_transformer,\n",
    "    vector_assembler_enhanced,\n",
    "    lr_classifier\n",
    "])\n",
    "\n",
    "# Train and evaluate\n",
    "model_lr_enhanced = pipeline_lr_enhanced.fit(reviews_train)\n",
    "predictions_lr_enhanced = model_lr_enhanced.transform(reviews_test).cache()\n",
    "\n",
    "print(\"\\n=== Logistic Regression with Enhanced Features - Results ===\")\n",
    "lr_enhanced_accuracy = evaluatorAcc.evaluate(predictions_lr_enhanced)\n",
    "lr_enhanced_f1 = evaluatorF1.evaluate(predictions_lr_enhanced)\n",
    "lr_enhanced_recall = evaluatorRecall.evaluate(predictions_lr_enhanced)\n",
    "lr_enhanced_precision = evaluatorPrecision.evaluate(predictions_lr_enhanced)\n",
    "\n",
    "print(f\"Accuracy: {lr_enhanced_accuracy:.4f}\")\n",
    "print(f\"F1: {lr_enhanced_f1:.4f}\")\n",
    "print(f\"Weighted Recall: {lr_enhanced_recall:.4f}\")\n",
    "print(f\"Weighted Precision: {lr_enhanced_precision:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6b3499",
   "metadata": {},
   "source": [
    "## Step 4: Text Content-Based Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ef7b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text processing pipeline with TF-IDF\n",
    "# 1. Tokenize the text\n",
    "tokenizer = RegexTokenizer(\n",
    "    inputCol=\"Text\",\n",
    "    outputCol=\"words\",\n",
    "    pattern=\"\\\\W\"\n",
    ")\n",
    "\n",
    "# 2. Remove stop words\n",
    "stop_words_remover = StopWordsRemover(\n",
    "    inputCol=\"words\",\n",
    "    outputCol=\"filtered_words\"\n",
    ")\n",
    "\n",
    "# 3. Create term frequency features\n",
    "hashing_tf = HashingTF(\n",
    "    inputCol=\"filtered_words\",\n",
    "    outputCol=\"raw_features\",\n",
    "    numFeatures=1000  # Hash to 1000 features\n",
    ")\n",
    "\n",
    "# 4. Apply IDF to get TF-IDF features\n",
    "idf = IDF(\n",
    "    inputCol=\"raw_features\",\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de8c79a",
   "metadata": {},
   "source": [
    "### Decision Tree with Text Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d121c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree with text content\n",
    "pipeline_dt_text = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    stop_words_remover,\n",
    "    hashing_tf,\n",
    "    idf,\n",
    "    dt_classifier\n",
    "])\n",
    "\n",
    "# Train and evaluate\n",
    "print(\"Training Decision Tree with text content...\")\n",
    "model_dt_text = pipeline_dt_text.fit(reviews_train)\n",
    "predictions_dt_text = model_dt_text.transform(reviews_test).cache()\n",
    "\n",
    "print(\"\\n=== Decision Tree with Text Content - Results ===\")\n",
    "dt_text_accuracy = evaluatorAcc.evaluate(predictions_dt_text)\n",
    "dt_text_f1 = evaluatorF1.evaluate(predictions_dt_text)\n",
    "dt_text_recall = evaluatorRecall.evaluate(predictions_dt_text)\n",
    "dt_text_precision = evaluatorPrecision.evaluate(predictions_dt_text)\n",
    "\n",
    "print(f\"Accuracy: {dt_text_accuracy:.4f}\")\n",
    "print(f\"F1: {dt_text_f1:.4f}\")\n",
    "print(f\"Weighted Recall: {dt_text_recall:.4f}\")\n",
    "print(f\"Weighted Precision: {dt_text_precision:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff172aa",
   "metadata": {},
   "source": [
    "### Logistic Regression with Text Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdf959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression with text content\n",
    "pipeline_lr_text = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    stop_words_remover,\n",
    "    hashing_tf,\n",
    "    idf,\n",
    "    lr_classifier\n",
    "])\n",
    "\n",
    "# Train and evaluate\n",
    "print(\"Training Logistic Regression with text content...\")\n",
    "model_lr_text = pipeline_lr_text.fit(reviews_train)\n",
    "predictions_lr_text = model_lr_text.transform(reviews_test).cache()\n",
    "\n",
    "print(\"\\n=== Logistic Regression with Text Content - Results ===\")\n",
    "lr_text_accuracy = evaluatorAcc.evaluate(predictions_lr_text)\n",
    "lr_text_f1 = evaluatorF1.evaluate(predictions_lr_text)\n",
    "lr_text_recall = evaluatorRecall.evaluate(predictions_lr_text)\n",
    "lr_text_precision = evaluatorPrecision.evaluate(predictions_lr_text)\n",
    "\n",
    "print(f\"Accuracy: {lr_text_accuracy:.4f}\")\n",
    "print(f\"F1: {lr_text_f1:.4f}\")\n",
    "print(f\"Weighted Recall: {lr_text_recall:.4f}\")\n",
    "print(f\"Weighted Precision: {lr_text_precision:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac7ad1b",
   "metadata": {},
   "source": [
    "## Results Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f42ca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"                    FINAL RESULTS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create results summary\n",
    "results = [\n",
    "    (\"Decision Tree - Simple (Text Length)\", dt_simple_accuracy, dt_simple_f1, dt_simple_precision, dt_simple_recall),\n",
    "    (\"Logistic Regression - Simple (Text Length)\", lr_simple_accuracy, lr_simple_f1, lr_simple_precision, lr_simple_recall),\n",
    "    (\"Decision Tree - Enhanced Features\", dt_enhanced_accuracy, dt_enhanced_f1, dt_enhanced_precision, dt_enhanced_recall),\n",
    "    (\"Logistic Regression - Enhanced Features\", lr_enhanced_accuracy, lr_enhanced_f1, lr_enhanced_precision, lr_enhanced_recall),\n",
    "    (\"Decision Tree - Text Content (TF-IDF)\", dt_text_accuracy, dt_text_f1, dt_text_precision, dt_text_recall),\n",
    "    (\"Logistic Regression - Text Content (TF-IDF)\", lr_text_accuracy, lr_text_f1, lr_text_precision, lr_text_recall)\n",
    "]\n",
    "\n",
    "print(f\"{'Model':<40} {'Accuracy':<10} {'F1':<10} {'Precision':<10} {'Recall':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for model_name, accuracy, f1, precision, recall in results:\n",
    "    print(f\"{model_name:<40} {accuracy:<10.4f} {f1:<10.4f} {precision:<10.4f} {recall:<10.4f}\")\n",
    "\n",
    "# Find best performing models\n",
    "best_accuracy = max(results, key=lambda x: x[1])\n",
    "best_f1 = max(results, key=lambda x: x[2])\n",
    "\n",
    "print(f\"\\nBest Accuracy: {best_accuracy[0]} ({best_accuracy[1]:.4f})\")\n",
    "print(f\"Best F1 Score: {best_f1[0]} ({best_f1[2]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f086ccf",
   "metadata": {},
   "source": [
    "## Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4facef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PERFORMANCE OBSERVATIONS:\")\n",
    "print(f\"   - Best overall accuracy: {best_accuracy[1]:.4f} ({best_accuracy[0]})\")\n",
    "print(f\"   - Best F1 score: {best_f1[2]:.4f} ({best_f1[0]})\")\n",
    "\n",
    "# Calculate improvement from simple to enhanced features\n",
    "dt_improvement = ((dt_enhanced_accuracy - dt_simple_accuracy) / dt_simple_accuracy) * 100\n",
    "lr_improvement = ((lr_enhanced_accuracy - lr_simple_accuracy) / lr_simple_accuracy) * 100\n",
    "\n",
    "print(f\"FEATURE ENGINEERING IMPACT:\")\n",
    "print(f\"   - Decision Tree improvement: {dt_improvement:+.1f}% (simple -> enhanced)\")\n",
    "print(f\"   - Logistic Regression improvement: {lr_improvement:+.1f}% (simple -> enhanced)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a106bc8c",
   "metadata": {},
   "source": [
    "## Sample Predictions Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b8bd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some sample predictions for analysis\n",
    "sample_predictions = predictions_lr_text.select(\n",
    "    \"Text\", \"label\", \"prediction\", \"helpfulness_ratio\", \"HelpfulnessNumerator\", \"HelpfulnessDenominator\"\n",
    ").limit(10)\n",
    "\n",
    "for row in sample_predictions.collect():\n",
    "    text_preview = row['Text'][:80] + \"...\" if len(row['Text']) > 80 else row['Text']\n",
    "    print(f\"Text: {text_preview}\")\n",
    "    print(f\"Helpfulness: {row['HelpfulnessNumerator']}/{row['HelpfulnessDenominator']} = {row['helpfulness_ratio']:.3f}\")\n",
    "    print(f\"Actual: {row['label']}, Predicted: {row['prediction']}\")\n",
    "    correct = \"âœ“\" if row['label'] == row['prediction'] else \"âœ—\"\n",
    "    print(f\"Result: {correct}\")\n",
    "    print(\"-\" * 100)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
