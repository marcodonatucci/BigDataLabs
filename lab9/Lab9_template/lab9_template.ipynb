{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, IndexToString, SQLTransformer\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import LongType, BooleanType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input path and constants\n",
    "inputData = \"/data/students/bigdata-01QYD/Lab9_DBD/Reviews.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# Create a DataFrame from Reviews.csv\n",
    "reviews = spark.read.load(inputData,\\\n",
    "                     format=\"csv\",\\\n",
    "                     header=True,\\\n",
    "                     inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the records with HelpfulnessDenominator>0 (i.e., rated reviews)\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and compute the value of Column label for the selected rated reviews\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe with Column label in training and test set\n",
    "(reviews_train, reviews_test) = ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create/Define the preprocessing steps and the classification algorithm you want to use \n",
    "# and the content of the pipeline that is used to train the model on reviews_train and apply it on reviews_test\n",
    "# Implement a first solution with one single values in features: text length\n",
    "...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit/Train the model\n",
    "model = pipeline.fit(reviews_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model on the test set\n",
    "predictions = model.transform(reviews_test).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics\n",
    "# Accuracy, F1, weighted recall, weighted precision\n",
    "evaluatorAcc = MulticlassClassificationEvaluator(labelCol=\"label\" , predictionCol= \"prediction\", metricName = \"accuracy\")\n",
    "evaluatorF1 = MulticlassClassificationEvaluator(labelCol=\"label\" , predictionCol= \"prediction\", metricName = \"f1\")\n",
    "evaluatorRecall = MulticlassClassificationEvaluator(labelCol=\"label\" , predictionCol= \"prediction\", metricName = \"weightedRecall\")\n",
    "evaluatorPrecision = MulticlassClassificationEvaluator(labelCol=\"label\" , predictionCol= \"prediction\", metricName = \"weightedPrecision\")\n",
    "\n",
    "print(\"Accuracy:\", evaluatorAcc.evaluate(predictions))\n",
    "print(\"F1:\", evaluatorF1.evaluate(predictions))\n",
    "print(\"Weighted Recall:\", evaluatorRecall.evaluate(predictions))\n",
    "print(\"Weighted Precision:\", evaluatorPrecision.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Compute the confusion matrix\n",
    "#                     Predicted  \n",
    "#  Actual       Useful   Useless\n",
    "#  Useful          A        B\n",
    "#  Useless          C        D\n",
    "\n",
    "A = predictions.filter(\"prediction=1 and label=1\").count()\n",
    "B = predictions.filter(\"prediction=0 and label=1\").count()\n",
    "C = predictions.filter(\"prediction=1 and label=0\").count()\n",
    "D = predictions.filter(\"prediction=0 and label=0\").count()\n",
    "\n",
    "print(\"                       Predicted\")\n",
    "print(\"  Actual \\t Useful\\tUseless\")\n",
    "print(\"  Useful \\t \"+str(A)+ \"\\t\\t\"+str(B))\n",
    "print(\"  Useless \\t \"+str(C)+ \"\\t\\t\"+str(D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision and recall for the two classes\n",
    "# Useful\n",
    "if A+C==0:\n",
    "    print(\"Precision(Useful): undefined\")\n",
    "else:\n",
    "    print(\"Precision(Useful):\"+str(A/(A+C)))\n",
    "    \n",
    "    \n",
    "print(\"Recall(Useful):\"+str(A/(A+B)))\n",
    "\n",
    "# Useless \n",
    "if B+D==0:\n",
    "    print(\"Precision(Useless): undefined\")\n",
    "else:\n",
    "    print(\"Precision(Useless):\"+str(D/(B+D)))\n",
    "    \n",
    "print(\"Recall(Useless):\"+str(D/(C+D)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
